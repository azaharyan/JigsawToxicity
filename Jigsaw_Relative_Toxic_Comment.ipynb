{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\nimport pandas as pd\nimport torch\nimport numpy as np\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport re\nfrom transformers import AutoTokenizer, AutoModel\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import mean_squared_error","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments_df = pd.read_csv('/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(comments_df.shape)\ncomments_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.shape)\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# May be add coefficients for each category as they are of different levels of severity\n\ndf['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) ).astype(int)\ndf['y'] = df['y']/df['y'].max()\n\ndf = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.y.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Undersample training data of level 0","metadata":{}},{"cell_type":"code","source":"min_len = (df['y'] >= 0.1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=201)\ndf = pd.concat([df[df['y'] >= 0.1], df_y0_undersample])\ndf['y'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_data_idx = df.shape[0]\nprint(total_data_idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged = pd.concat([df, comments_df], ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_merged.shape)\ndf_merged.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged[df_merged['text'].str.contains('http')]['text']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    '''\n    Cleans text. Permorms following operations:\n    1. Remove special symbols like #, &, etc\n    2. Remove extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+')\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'html.parser') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tqdm.pandas()\ndf_merged['text'] = df_merged['text'].progress_apply(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged['Fuck'] = df_merged['text'].str.contains('Fuck')\ndf_merged.replace({'Fuck':{False:0,True:1}},inplace = True)\nprint(df_merged['Fuck'].value_counts())\n\ndf_merged['Nigger'] = df_merged['text'].str.contains('Nigger')\ndf_merged.replace({'Nigger':{False:0,True:1}},inplace = True)\nprint(df_merged['Nigger'].value_counts())\n\ndf_merged['Suck'] = df_merged['text'].str.contains('Suck')\ndf_merged.replace({'Suck':{False:0,True:1}},inplace = True)\nprint(df_merged['Suck'].value_counts())\n\ndf_merged['Don'] = df_merged['text'].str.contains('Don')\ndf_merged.replace({'Don':{False:0,True:1}},inplace = True)\nprint(df_merged['Don'].value_counts())\n\ndf_merged['Shit'] = df_merged['text'].str.contains('Shit')\ndf_merged.replace({'Shit':{False:0,True:1}},inplace = True)\nprint(df_merged['Shit'].value_counts())\n\ndf_merged['Fat'] = df_merged['text'].str.contains('Fat')\ndf_merged.replace({'Fat':{False:0,True:1}},inplace = True)\nprint(df_merged['Fat'].value_counts())\n\ndf_merged['Gay'] = df_merged['text'].str.contains('Gay')\ndf_merged.replace({'Gay':{False:0,True:1}},inplace = True)\nprint(df_merged['Gay'].value_counts())\n\ndf_merged['Faggot'] = df_merged['text'].str.contains('Faggot')\ndf_merged.replace({'Faggot':{False:0,True:1}},inplace = True)\nprint(df_merged['Faggot'].value_counts())\n\ndf_merged['Moron'] = df_merged['text'].str.contains('Moron')\ndf_merged.replace({'Moron':{False:0,True:1}},inplace = True)\nprint(df_merged['Moron'].value_counts())\n\ndf_merged['Ass'] = df_merged['text'].str.contains('Ass')\ndf_merged.replace({'Ass':{False:0,True:1}},inplace = True)\nprint(df_merged['Ass'].value_counts())\n\ndf_merged['Cock'] = df_merged['text'].str.contains('Cock')\ndf_merged.replace({'Cock':{False:0,True:1}},inplace = True)\nprint(df_merged['Cock'].value_counts())\n\ndf_merged['Jew'] = df_merged['text'].str.contains('Jew')\ndf_merged.replace({'Jew':{False:0,True:1}},inplace = True)\nprint(df_merged['Jew'].value_counts())\n\ndf_merged['Pig'] = df_merged['text'].str.contains('Pig')\ndf_merged.replace({'Pig':{False:0,True:1}},inplace = True)\nprint(df_merged['Pig'].value_counts())\n\ndf_merged['Stupid'] = df_merged['text'].str.contains('Stupid')\ndf_merged.replace({'Stupid':{False:0,True:1}},inplace = True)\nprint(df_merged['Stupid'].value_counts())\n\ndf_merged['Die'] = df_merged['text'].str.contains('Die')\ndf_merged.replace({'Die':{False:0,True:1}},inplace = True)\nprint(df_merged['Die'].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BertSequenceVectorizer:\n    def __init__(self):\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model_name = \"../input/roberta-base\"\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n     \n#         self.bert_model = transformers.RobertaModel.from_pretrained(self.model_name)  \n        self.bert_model = AutoModel.from_pretrained(self.model_name)      \n        self.bert_model = self.bert_model.to(self.device)\n        self.max_len = 128\n#         self.max_len = 256\n        \n    def vectorize(self, sentence : str) -> np.array:\n        inp = self.tokenizer.encode(sentence)\n        len_inp = len(inp)\n\n        if len_inp >= self.max_len:\n            inputs = inp[:self.max_len]\n            masks = [1] * self.max_len\n        else:\n            inputs = inp + [0] * (self.max_len - len_inp)\n            masks = [1] * len_inp + [0] * (self.max_len - len_inp)\n\n        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)\n        masks_tensor = torch.tensor([masks], dtype=torch.long).to(self.device)\n        \n        seq_out = self.bert_model(inputs_tensor, masks_tensor)[0]\n        pooled_out = self.bert_model(inputs_tensor, masks_tensor)[1]\n\n        if torch.cuda.is_available():    \n            return seq_out[0][0].cpu().detach().numpy() \n        else:\n            return seq_out[0][0].detach().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BSV = BertSequenceVectorizer()\ndf_merged['text'] = df_merged['text'].progress_apply(lambda x: BSV.vectorize(x) if x is not np.nan else np.array([0]*768))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert = pd.DataFrame(df_merged['text'].tolist())\nbert.columns = ['text_bertvec_'+str(col) for col in bert.columns]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_bert_df = pd.DataFrame(bert)\ntext_bert_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_merged.reset_index(drop=True, inplace=True)\ndf_merged.head()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_df = pd.concat([df_merged, text_bert_df], axis=1)\nconcat_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_cols = ['Fuck', 'Nigger', 'Suck', 'Don', 'Shit', 'Fat',\n            'Gay', 'Faggot', 'Moron', 'Ass','Cock','Jew','Pig','Stupid','Die']\nnum_cols = list(list(text_bert_df.columns)) \nfeat_cols = cat_cols + num_cols\n#feat_cols = num_cols\nTARGET = 'y'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = concat_df.iloc[:25000, :]\nval_df = concat_df.iloc[25000:total_data_idx, :]\ntest_df = concat_df.iloc[total_data_idx:, :]\nprint(train_df.shape, val_df.shape, test_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x = train_df[feat_cols]\ntrain_y = train_df[TARGET]\nval_x = val_df[feat_cols]\nval_y = val_df[TARGET]\ntest_x = test_df[feat_cols]\ntest_y = test_df[TARGET]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_x.shape)\nprint(train_y.shape)\nprint(val_x.shape)\nprint(val_y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#params = {   \n#    'max_depth': 7,\n#    'learning_rate': 0.05,\n#    'n_estimators': 400\n#}\n\n#train_data = xgb.DMatrix(train_x, label=train_y)\n#val_data = xgb.DMatrix(val_x, label=val_y)\n\n#model = xgb.train(\n#    params,\n#    train_data\n#)\n\n#val_pred = model.predict(val_data)\n\n#pred_df = pd.DataFrame(sorted(zip(val_x.index, val_pred, val_y)), columns=['index', 'predict', 'actual'])\nparams = {   \n    'objective': 'regression',\n#     'objective': 'regression_l1',\n    'metric': 'rmse',\n    'num_leaves': 32,\n    'max_depth': 7,\n    \"feature_fraction\": 0.8,\n    'subsample_freq': 1,\n    \"bagging_fraction\": 0.98,\n    'min_data_in_leaf': 2,\n    'learning_rate': 0.05,\n    \"boosting\": \"gbdt\",\n    \"lambda_l1\": 0.2,\n    \"lambda_l2\": 10,\n    \"verbosity\": -1,\n    \"random_state\": 42,\n    \"num_boost_round\": 8000,\n    \"early_stopping_rounds\": 100\n}\n\ntrain_data = lgb.Dataset(train_x, label=train_y)\nval_data = lgb.Dataset(val_x, label=val_y)\n\nmodel = lgb.train(\n    params,\n    train_data, \n    categorical_feature = cat_cols,\n    valid_names = ['train', 'valid'],\n    valid_sets =[train_data, val_data], \n    verbose_eval = 100,\n)\n\nval_pred = model.predict(val_x, num_iteration=model.best_iteration)\n\npred_df = pd.DataFrame(sorted(zip(val_x.index, val_pred, val_y)), columns=['index', 'predict', 'actual'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df[pred_df['actual'] != 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_data = xgb.DMatrix(test_x)\ntest_data = lgb.Dataset(test_x)\ntest_pred = model.predict(test_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments_df['score'] = test_pred ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments_df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}